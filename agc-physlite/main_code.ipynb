{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d3d418-e712-4ee2-a583-6de91f346c28",
   "metadata": {},
   "source": [
    "# Analysis Grand Challenge of the PHYSLITE data format\n",
    "\n",
    "This notebook is written as the result of a project of the IRIS-HEP summer fellowship programme. The web page, dedicated to the project, can be found via the [link](https://iris-hep.org/fellows/Denys-Klekots.html).\n",
    "\n",
    "The code below is heavily based on the Analysis Grand Challenge (AGC) of the CMS Open Data $t\\bar{t}$. There is a link to the corresponding [GitHub repository](https://github.com/alexander-held/PyHEP-2022-AGC) written by Alexander Held.\n",
    "\n",
    "Here we will use scientific Python infrastructure, which allows one to read data from the file and further analyse it. The data in high-energy physics is usually stored on an event-by-event basis and contains information on the detected particles. As there is an arbitrary number of particles for each event, we will use the `awkward` Python tools which allow us to handle arrays with an arbitrary length. The PHYSLITE format is stored inside a widely used in particle physics `*.root` file format and can be read by using the `coffee` python package. Also, the different events in high-energy physics are independent of each other, which allows for parallel computation and distribution over the cluster, the `dask` python package allows one to handle parallel computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1eb49c-2d2f-45a3-924f-1d7da14131ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "import numpy as np\n",
    "import hist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, PHYSLITESchema\n",
    "from coffea import processor\n",
    "\n",
    "\n",
    "from dask.distributed import Client, performance_report\n",
    "\n",
    "# create a folder for output tracking of uproot.open setup\n",
    "MEASUREMENT_PATH = Path(datetime.datetime.now().strftime(\"measurements/%Y-%m-%d_%H-%M-%S\"))\n",
    "os.makedirs(MEASUREMENT_PATH)\n",
    "\n",
    "# Set up the client distribution calculation for coffea-opendata.casa. Change the URL for UChicago\n",
    "client = Client(\"tls://localhost:8786\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b7211-6c29-4a2d-a1d6-1c5c810f98aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "The PHYSLITE format files used for this analysis contain a lot of information, organised in brunches, a description of which can be accessed via the [link](https://atlas-physlite-content-opendata.web.cern.ch/). For the purpose of this specific analysis, a few of them are used and a filter is applied to select used ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6e5fe-af3c-4cb2-9567-70d51b1c206a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_name(name):\n",
    "    return name in [\n",
    "        \"AnalysisElectronsAuxDyn.pt\",\n",
    "        \"AnalysisElectronsAuxDyn.eta\",\n",
    "        \"AnalysisElectronsAuxDyn.phi\",\n",
    "        \"AnalysisElectronsAuxDyn.m\",\n",
    "        \n",
    "        \"AnalysisMuonsAuxDyn.pt\",\n",
    "        \"AnalysisMuonsAuxDyn.eta\",\n",
    "        \"AnalysisMuonsAuxDyn.phi\",\n",
    "        \"AnalysisMuonsAuxDyn.m\",\n",
    "        \n",
    "        \"AnalysisJetsAuxDyn.pt\",\n",
    "        \"AnalysisJetsAuxDyn.eta\",\n",
    "        \"AnalysisJetsAuxDyn.phi\",\n",
    "        \"AnalysisJetsAuxDyn.m\",\n",
    "        \n",
    "        \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pb\",\n",
    "        \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pc\",\n",
    "        \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pu\",\n",
    "        \n",
    "        \"EventInfoAuxDyn.mcEventWeights\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71381b99-ac0e-4d0c-bfe2-b6387c8b08cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "The code in the cell below will read the data from the file in a function `NanoEventsFactory.from_root(...)` and produce an awkward array, named `events` in this code. Here the `PHYSLITESchema` is specified to correspond to the format of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1525911-4af1-4721-ac2a-56bd0ad4f526",
   "metadata": {},
   "source": [
    "The file used in the next cell is one of the Montecarlo-generated files for the $t \\bar{t}$ pair physical process of nominal variation. See below for more information and a reference to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a46d3-249b-459b-92f5-fb708c59160e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mc_file = \"root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.37620644._000012.pool.root.1\"\n",
    "\n",
    "events = NanoEventsFactory.from_root(\n",
    "    {mc_file: \"CollectionTree\"},\n",
    "    schemaclass=PHYSLITESchema,\n",
    "    iteritems_options=dict(filter_name=filter_name),\n",
    ").events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5744c0-b9f7-4bf8-8e85-95bcc99c80b0",
   "metadata": {},
   "source": [
    "### B-tagging discriminant\n",
    "\n",
    "The DL1 flavour tagging algorithm is used for the pre-analysis of data, stored in the PHYSLITE format used in this analysis. The output of the DL1 algorithm is the $p_b$, $p_c$ and $p_u$ variables that are combined by the following formula to define the final discriminant on b-tagging\n",
    "\n",
    "$$\n",
    "D_{DL1} = log \\left( \\frac{p_b}{f_c \\cdot p_c + (1-f_c) \\cdot p_u} \\right)\n",
    "$$ \n",
    "\n",
    "The $p_b$, $p_c$ and $p_u$ are stored in the input file under fields (also known as brunches) named `BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pb`, `BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pc` and `BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pu` respectively. For more information about discriminant value please refer to the following [link](https://ftag.docs.cern.ch/recommendations/algs/2019-recommendations/#algorithm-structure) (Please note that the CERN account might be needed for access). $f_c$ is the constant which is equal to $f_c = 0.018$ in this analysis.\n",
    "\n",
    "The jet is considered as b-tagged if the $D_{DL1}$ variable is above threshold. The threshold value of `2.456` was used here, which correspond to efficiency of 77%. Refer to the [link](https://ftag.docs.cern.ch/recommendations/algs/r22-preliminary/#working-point-definition-for-dl1dv01) for more information (please note that CERN account might be needed to access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430fa53-191b-49e1-8c94-c657b4ebcf68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_jets_btagDL1d(events):\n",
    "    \n",
    "    BTagging = events.BTagging_AntiKt4EMPFlow\n",
    "    \n",
    "    f_c = 0.018\n",
    "    DDL1 = BTagging.DL1dv01_pb/(f_c*BTagging.DL1dv01_pc + (1-f_c)*BTagging.DL1dv01_pu)\n",
    "    DDL1 = np.log(DDL1)\n",
    "\n",
    "    return DDL1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18310875-60ae-4d0a-b73e-fff01c4dd882",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Event selections and observables\n",
    "\n",
    "The key point of this analysis's grand challenge is to make a benchmark of the PHYSLITE data format on the example of the study of the t quark production. All events in this analysis go through preselection stages which include the selection of transverse momentum ($p_t$) and the $\\eta$ parameter of the leptons and jets.\n",
    "Additional selections depend on the observable. There are two observables in this analysis:\n",
    "1) The mass of the trijet with the largest $p_t$ value among events which contain at least four jets, at least two of them must be b-tagged and exactly one charged lepton. From all possible combinations of jets grouped in three, the one group with the largest transverse momentum (vector sum) is chosen and invariant mass is calculated for this group of jets. Additionally, at least one b-tagged jet must be present in the group of three.\n",
    "2) The $H_T$ observable value is the scalar sum of the transverse momentum of all jets in events. For this observable, the selected events must have at least four jets, exactly one of them must be b-tagged, and exactly one lepton must be present in the event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22c53f-6a27-4599-b522-995da8990d27",
   "metadata": {},
   "source": [
    "The schematic view of the event with four jets (like the one used for the calculation of trijet mass) is presented in the following image. Here two b quarks are forming jets that are b-tagged. The other two quarks (which come from W boson decay) are not necessarily b-tagged. The charged lepton is also coming from the decay of other W bosons, and the neutrino is not detected in this experiment.\n",
    "\n",
    "<div>\n",
    "<img src=\"utils/ttbar.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "The source of the image is the [AGC](https://github.com/alexander-held/PyHEP-2022-AGC/blob/main/talk.ipynb) notebook by Alexander Held."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c1584-d382-4117-a4bb-824c1a84816b",
   "metadata": {},
   "source": [
    "### Notes on b-tagging variables in code\n",
    "\n",
    "At the moment of writing of this code, the jets information are stored in the file under `AnalysisJetsAuxDyn` brunch name, at the same time the variables, used for calculation of the b-tagging discriminant, are stored in the brunch named `BTagging_AntiKt4EMPFlowAuxDyn` which is slightly inconvenient as one might prefer to have all jet information inside a single brunch. In the future, it might be added the possibility to the `PHYSLITESchema` to link the b-tagging info into the jet's branch, but for now, it is done manually in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e147a5-5be8-4141-aacd-f266c6d4304e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "B_TAG_THRESHOLD = 2.456\n",
    "\n",
    "def calculate_preselections(events):\n",
    "    # pT > 30 GeV and abs(etha)<2.1 for leptons\n",
    "    selected_electrons = events.Electrons[events.Electrons.pt > 30000 & (np.abs(events.Electrons.eta) < 2.1)]\n",
    "    selected_muons = events.Muons[events.Muons.pt > 30000 & (np.abs(events.Muons.eta) < 2.1)]\n",
    "    \n",
    "    # calculate tagging variable and manually attach it to jets (which passed the preselection)\n",
    "    jets = events.Jets\n",
    "    jets[\"btagDL1d\"] = calculate_jets_btagDL1d(events)\n",
    "    \n",
    "    # pT > 25 GeV and abs(etha) < 2.4 for jets\n",
    "    selected_jets = jets[events.Jets.pt > 25000 & (np.abs(events.Jets.eta) < 2.4)] \n",
    "    \n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    \n",
    "    return event_filters, selected_jets\n",
    "    \n",
    "def calculate_trijet_mass_and_ev_filter(event_filters, selected_jets):    \n",
    "        \n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagDL1d > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "    \n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "    \n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagDL1d, np.maximum(trijet.j2.btagDL1d, trijet.j3.btagDL1d))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "        \n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    \n",
    "    \n",
    "    return ak.flatten(trijet_mass), event_filters\n",
    "\n",
    "\n",
    "def calculate_Ht_and_ev_filter(event_filters, selected_jets):\n",
    "    \n",
    "    # exactly ont b-tagged jets (\"tag\" means score above threshold)\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagDL1d > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "    \n",
    "    observable_Ht = ak.sum(selected_jets.pt, axis=-1)\n",
    "    \n",
    "    return observable_Ht, event_filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f529db-1470-4961-bbd8-4d92a5c26b32",
   "metadata": {},
   "source": [
    "The code below does fill histograms for the trijet mass and the $H_t$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0e840-0390-442c-8f7e-4080fbaf5b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_filters, selected_jets = calculate_preselections(events)\n",
    "\n",
    "reconstructed_top_mass, _ = calculate_trijet_mass_and_ev_filter(event_filters, selected_jets)\n",
    "hist_reco_mtop = hist.Hist.new.Reg(25, 50, 550, label=\"$m_{bjj} [GeV]$\").Double().fill(reconstructed_top_mass/1000)\n",
    "\n",
    "observable_Ht, _ = calculate_Ht_and_ev_filter(event_filters, selected_jets)\n",
    "hist_Ht = hist.Hist.new.Reg(25, 70, 570, label=\"$H_T$ [GeV/c]\").Double().fill(observable_Ht/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85939ab4-df2f-43a7-9876-a8c681377ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize\n",
    "artists = hist_reco_mtop.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d217c0-74b2-4595-8f9e-e5f56f95bee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize\n",
    "artists = hist_Ht.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44805789-7fb2-445a-ae52-b126e39b5193",
   "metadata": {},
   "source": [
    "# Distributed computations\n",
    "\n",
    "Now we will move to the distributed computations. The code below applies the same approach to the selection of the data and calculation of the mass of the trijets and $H_T$. The further code uses multiple PHYSLITE files which have the same structure of branches and are processed in a parallel way, after which the results are merged in the final histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4741a-e1e7-4df2-9c86-5ee5d1cdb1c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "The files used for distributed calculations and their metadata are specified in the cell below. This particular case uses Monte Carlo generated data which can be accessed via the [link](https://opendata.cern.ch/record/80017) (and the [link](https://opendata.cern.ch/record/80010) for physical processes labellet as the \"W and jets\" in below) and are `nominal` (as named in variation). To estimate the systematic uncertainties, the files from other Monte Carlo simulations are processed as well, those files are marked as `systematic_var` in variation and can be found via the [link](https://opendata.cern.ch/record/80018). The corresponding metadata (for both Monte Carlo simulation) can be accessed via the [link](https://opendata.atlas.cern/docs/documentation/overview_data/data_research_2024/#metadata). Please refer to the comments in the `file_utils.py` file for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fadf1-e58a-4b72-8c8c-b2cbe495edc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "The metadata is read from the `metadata.csv` file during the importing of `file_utils`. All of the available metadata for specific processes is stored, but only some of them are used (see below), this redundancy affects performance very little but is much more favourable from the code flexibility and user-friendliness point of view. \n",
    "\n",
    "Additionally, the label of physicall process and variation are added to the metadata dictionary in the cell below, the label of physicall process is used during the drawing of the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d67087-15aa-4429-8d5b-4e64a63168d2",
   "metadata": {},
   "source": [
    "## Monte Carlo-simulated physical processes of $t$ quarks producing.\n",
    "\n",
    "The processes of producing single top quarks and mater-antimatter $t \\bar{t}$ pairs were simulated in the input data for this analysis. The Feynman diagrams of the leading order for single top processes are described in the picture. The original of the picture is Ref. [1]. Please note the primary goal of this notebook is to provide an Analysis Grand Challenge on the technical aspects of the work of the code and used Python modules, so we do not dive deep into the discussion of the physical part of the Montecarlo simulation but rather use the already generated data in PHYSLITE format to test the software.\n",
    "\n",
    "<div>\n",
    "<img src=\"utils/Single_quark_process_diagrams.png\" width=\"1000\"/>\n",
    "</div>\n",
    "The diagrams shows single top production: (a) - t-channel, (b) - Wt -channel, (c) s-channel.\n",
    "\n",
    "\n",
    "[1] Precision Measurements of Top Quark Production with the ATLAS Detector. Philipp  Stolte. EPJ Web Conf. 137 08015 (2017). [DOI:10.1051/epjconf/201713708015](https://doi.org/10.1051/epjconf/201713708015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dff7f0-51a3-46b0-92a0-049a55f411bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import file_utils\n",
    "\n",
    "XCACHE_PREFIX = \"root://xcache.af.uchicago.edu:1094//\" # This could be an empty string or one may like to add some prefix, for example, xcache or so.\n",
    "\n",
    "\n",
    "fileset = {\n",
    "#############################\n",
    "## singletop samples    \n",
    "#############################    \n",
    "\"PowhegPythia8EvtGen_A14_singletop_schan_lept_top\"  : \n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_PowhegPythia8EvtGen_A14_singletop_schan_lept_top},\n",
    "            'metadata': {**file_utils.metadata_PowhegPythia8EvtGen_A14_singletop_schan_lept_top, \"process-label\":\"single top s chan.\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"PowhegPythia8EvtGen_A14_singletop_schan_lept_antitop\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_PowhegPythia8EvtGen_A14_singletop_schan_lept_antitop},\n",
    "            'metadata': {**file_utils.metadata_PowhegPythia8EvtGen_A14_singletop_schan_lept_antitop, \"process-label\":\"single top s chan.\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"PhPy8EG_A14_tchan_BW50_lept_top\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_PhPy8EG_A14_tchan_BW50_lept_top},\n",
    "            'metadata': {**file_utils.metadata_PhPy8EG_A14_tchan_BW50_lept_top, \"process-label\":\"single top t chan.\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"PhPy8EG_A14_tchan_BW50_lept_antitop\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_PhPy8EG_A14_tchan_BW50_lept_antitop},\n",
    "            'metadata': {**file_utils.metadata_PhPy8EG_A14_tchan_BW50_lept_antitop, \"process-label\":\"single top t chan.\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"PhPy8EG_tW_dyn_DR_incl_antitop\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_PhPy8EG_tW_dyn_DR_incl_antitop},\n",
    "            'metadata': {**file_utils.metadata_PhPy8EG_tW_dyn_DR_incl_antitop, \"process-label\":\"single top tW chan.\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"PhPy8EG_tW_dyn_DR_incl_top\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_PhPy8EG_tW_dyn_DR_incl_top},\n",
    "            'metadata': {**file_utils.metadata_PhPy8EG_tW_dyn_DR_incl_top, \"process-label\":\"single top tW chan.\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "#############################\n",
    "## t-tbar samples    \n",
    "#############################\n",
    "\"PhPy8EG_A14_ttbar_hdamp258p75_nonallhad\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_PhPy8EG_A14_ttbar_hdamp258p75_nonallhad},\n",
    "            'metadata': {**file_utils.metadata_PhPy8EG_A14_ttbar_hdamp258p75_nonallhad, \"process-label\":\"$t \\\\bar{t}$\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"PowhegHerwig7EvtGen_tt_hdamp258p75_713_SingleLep\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_PowhegHerwig7EvtGen_tt_hdamp258p75_713_SingleLep},\n",
    "            'metadata': {**file_utils.metadata_PowhegHerwig7EvtGen_tt_hdamp258p75_713_SingleLep, \"process-label\":\"$t \\\\bar{t}$\", \"variation\":\"systematic_var\"},\n",
    "            },\n",
    "#############################\n",
    "## W and jets samples    \n",
    "#############################\n",
    "\"Sh_2211_Wenu_maxHTpTV2_BFilter\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wenu_maxHTpTV2_BFilter},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wenu_maxHTpTV2_BFilter, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wenu_maxHTpTV2_CFilterBVeto\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wenu_maxHTpTV2_CFilterBVeto},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wenu_maxHTpTV2_CFilterBVeto, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wenu_maxHTpTV2_CVetoBVeto\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wenu_maxHTpTV2_CVetoBVeto},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wenu_maxHTpTV2_CVetoBVeto, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wmunu_maxHTpTV2_BFilter\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wmunu_maxHTpTV2_BFilter},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wmunu_maxHTpTV2_BFilter, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wmunu_maxHTpTV2_CFilterBVeto\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wmunu_maxHTpTV2_CFilterBVeto},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wmunu_maxHTpTV2_CFilterBVeto, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wmunu_maxHTpTV2_CVetoBVeto\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wmunu_maxHTpTV2_CVetoBVeto},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wmunu_maxHTpTV2_CVetoBVeto, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wtaunu_L_maxHTpTV2_BFilter\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wtaunu_L_maxHTpTV2_BFilter},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wtaunu_L_maxHTpTV2_BFilter, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wtaunu_L_maxHTpTV2_CFilterBVeto\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wtaunu_L_maxHTpTV2_CFilterBVeto},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wtaunu_L_maxHTpTV2_CFilterBVeto, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wtaunu_L_maxHTpTV2_CVetoBVeto\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wtaunu_L_maxHTpTV2_CVetoBVeto},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wtaunu_L_maxHTpTV2_CVetoBVeto, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wtaunu_H_maxHTpTV2_BFilter\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wtaunu_H_maxHTpTV2_BFilter},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wtaunu_H_maxHTpTV2_BFilter, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wtaunu_H_maxHTpTV2_CFilterBVeto\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wtaunu_H_maxHTpTV2_CFilterBVeto},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wtaunu_H_maxHTpTV2_CFilterBVeto, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            },\n",
    "\"Sh_2211_Wtaunu_H_maxHTpTV2_CVetoBVeto\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_Sh_2211_Wtaunu_H_maxHTpTV2_CVetoBVeto},\n",
    "            'metadata': {**file_utils.metadata_Sh_2211_Wtaunu_H_maxHTpTV2_CVetoBVeto, \"process-label\":\"W and jets\", \"variation\":\"nominal\"},\n",
    "            }\n",
    "}      \n",
    "    \n",
    "# Let's print the description from the metadata.\n",
    "for file_dict in fileset.values():\n",
    "    \n",
    "    print(\"Physics: {phys:55s}| Description: {desc:s}\".format(phys=file_dict[\"metadata\"][\"physics_short\"], desc=file_dict[\"metadata\"][\"description\"] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b627e-34c8-4bb7-af6a-d7e490be1d15",
   "metadata": {},
   "source": [
    "The next cell sets up a \"runner\" to parallelize our computations on a cluster using dask. It also sets up the chunk size to be roughly 100000 events and tells the runner to skip the files that were not accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82bcae-1c65-4955-8d1f-f287c4990844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up runner\n",
    "\n",
    "run = processor.Runner(\n",
    "    executor = processor.DaskExecutor(client=client, compression=None),\n",
    "    schema=PHYSLITESchema,\n",
    "    savemetrics=True,\n",
    "    chunksize=100_000,\n",
    "    skipbadfiles=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b07271-e33d-4703-aaf0-ad3ef99a23da",
   "metadata": {},
   "source": [
    "Preprocessing checks all the files in our input fileset for access, splits them into chunks of roughly `chunksize` and removes that chunks that were not accessible. It returns a generator that generates chunks of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303da95a-7c89-4973-bd71-608b57e2c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# pre-process\n",
    "samples = run.preprocess(fileset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312a216-220d-4cc6-bc53-86ceafdeffb5",
   "metadata": {},
   "source": [
    "The code in the cell below defines the processing of a single input file, which is processed in parallel with others. As was mentioned above, the data processed in this particular example is Montecarlo-generated and is for several different physical processes. The number of events generated for the specific physical process is arbitrary, to correctly show the different processes on the same histogram one needs to normalise data to the same luminosity, which is in our case `LUMINOSITY = 36640`. The numerical value of luminosity (in inverse picobarns) used here corresponds to the one of data taken by the ATLAS collaboration during 2015 and 2016 (see Table 8 of the following [publication](https://arxiv.org/abs/2212.09379) for reference). It is worth mentioning that the events in this Monte Carlo data are weighted as well. \n",
    "\n",
    "The following formula was used to normalize data with weighted events to respond to the given luminosity.\n",
    "\n",
    "`lumi_weight = LUMINOSITY * cross-section * k-factor * filter-efficiency / sum-of-mc-weights`\n",
    "\n",
    "Here `cross-section`, `k-factor` and `filter-efficiency` are defined in metadata. The `sum-of-mc-weights` is calculated from the values stored in the PHYSLITE file as the sum of Montecarlo weights for each physical process.\n",
    "\n",
    "The weight of each event during the filling histogram is `weight=lumi_weight*mc_weight`, where `lumi_weight` is constant and `mc_weight` is event specific (it is stored in the PHYSLITE file under `EventInfo.mcEventWeights` brunch). \n",
    "\n",
    "In this code, the Monte Carlo-generated data for a single physical process can be split between multiple PHYSLITE files, each processed in parallel. For normalisation of the data one needs to divide weights on the `sum-of-mc-weights` across all events for a given physical process, but as the files are processed in parallel it is much more efficient to return the `sum-of-mc-weights` from the function which is doing the processing of each single PHYSLITE file and divide each bin of the histogram on the `sum-of-mc-weights` outside of the parallel code. This approach is applied in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b01ad-f3ec-4820-ada3-fe3d3976862b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LUMINOSITY = 36640 \n",
    "\n",
    "class create_histograms(processor.ProcessorABC):\n",
    "    def process(self, events):\n",
    "            \n",
    "        # Fetching metadata\n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        cross_section = events.metadata[\"crossSection_pb\"]\n",
    "        filter_efficiency = events.metadata[\"genFiltEff\"]\n",
    "        k_factor = events.metadata[\"kFactor\"]\n",
    "        process_label = events.metadata[\"process-label\"]\n",
    "        variation = events.metadata[\"variation\"]\n",
    "        \n",
    "        # Calculate the sum of weights for the currently processing PHYSELITE file.\n",
    "        sum_of_mc_weights = ak.sum(events.EventInfo.mcEventWeights[:, 0])\n",
    "        \n",
    "        # calculate lumi_weight constant term\n",
    "        lumi_weight = LUMINOSITY * cross_section * k_factor * filter_efficiency\n",
    "                \n",
    "        # Retrieve events and jet's preselection\n",
    "        event_filters, selected_jets = calculate_preselections(events)\n",
    "        \n",
    "        ##############################################\n",
    "        ## Trijet mass calculation\n",
    "        ##############################################\n",
    "        reconstructed_top_mass, event_filters_mass = calculate_trijet_mass_and_ev_filter(event_filters, selected_jets)    \n",
    "        mc_weight_mass = events.EventInfo.mcEventWeights[:, 0][event_filters_mass]\n",
    "        \n",
    "        hist_reco_mtop = (hist.Hist.new.Reg(25, 50, 550, name=\"m_reco_top\", label=r\"$m_{bjj}$ [GeV]\")\n",
    "                         .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                         .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "                         .Weight()\n",
    "                         .fill(reconstructed_top_mass/1000, weight=lumi_weight*mc_weight_mass, process=process_label, variation=variation) )\n",
    "        \n",
    "        ##############################################\n",
    "        ## Ht calculation                             \n",
    "        ##############################################\n",
    "        observable_Ht, event_filters_Ht = calculate_Ht_and_ev_filter(event_filters, selected_jets)\n",
    "        mc_weight_Ht = events.EventInfo.mcEventWeights[:, 0][event_filters_Ht] \n",
    "        \n",
    "        hist_Ht = (hist.Hist.new.Reg(25, 70, 570, name=\"H_t\", label=r\"$H_T$ [GeV/c]\")\n",
    "                         .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                         .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "                         .Weight()\n",
    "                         .fill(observable_Ht/1000, weight=lumi_weight*mc_weight_Ht, process=process_label, variation=variation) )\n",
    "        \n",
    "        \n",
    "        hists = {\"hist_reco_mtop\":hist_reco_mtop, \"hist_Ht\":hist_Ht, \"sum_of_file_weights\": sum_of_mc_weights}\n",
    "        return {dataset: hists}\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff439aa-1997-4cd6-82f7-3cbaef759f17",
   "metadata": {},
   "source": [
    "The cell below maps and computes the dedicated processor, `create_histograms` in our case, over all the chunks in samples while paralellizing the procedure.\n",
    "There we also specify branch filters and usage of `PHYSLITESchema`\n",
    "Here we divide the counts in histogram bins by the corresponding `sum-of-mc-weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1bee4-76ce-49b5-8c5a-45da677dc6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# execute\n",
    "with performance_report(filename=MEASUREMENT_PATH/\"dask-report-compute.html\"):\n",
    "    out, report = run(\n",
    "        samples,\n",
    "        processor_instance=create_histograms(),\n",
    "        iteritems_options=dict(filter_name=filter_name),\n",
    "    )\n",
    "\n",
    "    # Dividing the number of counts in each bin in the histogram by the sum of weights.\n",
    "    for channel in out.keys():\n",
    "        out[channel][\"hist_reco_mtop\"] /= out[channel][\"sum_of_file_weights\"]\n",
    "        out[channel][\"hist_Ht\"] /= out[channel][\"sum_of_file_weights\"]\n",
    "        \n",
    "        # Delete the sum of weights; we will not use it anymore, which helps avoid confusion.\n",
    "        del out[channel][\"sum_of_file_weights\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950b05f-9f4a-4b73-922f-39476e372989",
   "metadata": {},
   "source": [
    "Let's print the reporting information of efficiency of parallel computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c2210-0f80-4c6c-ba45-aedd30a078bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3dfa3-1283-4858-a6de-68ba86538ff3",
   "metadata": {},
   "source": [
    "And finally making the histogram from the distributed computing. Here only the nominal variation is to be drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e52294-6589-4936-bcdf-dbddced6725b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_histogram_hist_reco_mtop = sum([v[\"hist_reco_mtop\"] for v in out.values()])\n",
    "\n",
    "\n",
    "# Draw the filled stacked histograms\n",
    "artists = full_histogram_hist_reco_mtop[:, :, \"nominal\"].stack(\"process\").plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\",\n",
    ")\n",
    "\n",
    "\n",
    "# Draw the same histogram in the \"step\" style to get error bars on the same plot.\n",
    "artists = full_histogram_hist_reco_mtop[:, :, \"nominal\"].stack(\"process\").plot(\n",
    "    stack=True, histtype=\"step\", linewidth=1, label=\"\", color=\"black\", yerr=True\n",
    ")\n",
    "\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201796a-db4a-4740-9e3c-bafbed326238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_histogram_hist_Ht = sum([v[\"hist_Ht\"] for v in out.values()])\n",
    "\n",
    "\n",
    "# Draw the filled stacked histograms\n",
    "artists = full_histogram_hist_Ht[:, :, \"nominal\"].stack(\"process\").plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\",\n",
    ")\n",
    "\n",
    "\n",
    "# Draw the same histogram in the \"step\" style to get error bars on the same plot.\n",
    "artists = full_histogram_hist_Ht[:, :, \"nominal\"].stack(\"process\").plot(\n",
    "    stack=True, histtype=\"step\", linewidth=1, label=\"\", color=\"black\", yerr=True\n",
    ")\n",
    "\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd72460-fce6-4cc3-bf94-843f7114a0f0",
   "metadata": {},
   "source": [
    "## Save histograms into the file\n",
    "\n",
    "The code in the next cell will save the histograms built on the data for each physical observable (also channel here), physical process and variation. \n",
    "Please note that there are different filesets with the same process label, for example, filesets named `singletop_schan_lept_top` and `singletop_schan_lept_antitop` have the same process label, which is `single top s chan`. The histograms for such filsets are merged in the cell below. \n",
    "\n",
    "First, the code below iterates through all histograms storing the histogram, its channel, physical process (also named sample in code) and variation in the `histograms_collection` list. In the second step, the histograms from histograms_collection are stored in the .root file by the following path \"channel/sample/variation\".\n",
    "\n",
    "The cabinetry is usually used with Montecarlo-generated data and corresponding measured data to calculate systematic uncertainties, but as this is an example code we used pseudo data instead of measured data. The pseudodata histogram in our case is merged histograms from all physical processes (samples) of nominal variation where stochastically noice was introduced for the count in each bit with a Gaussian distribution (as an approximation of the Poisson distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1678276-31f0-4b66-8e9f-359f128634a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_FILE_NAME = \"histograms.root\"\n",
    "\n",
    "# Saving the histograms of each channel into the file.\n",
    "with uproot.recreate(HIST_FILE_NAME) as f:\n",
    "        \n",
    "        \n",
    "    histograms_collection = []\n",
    "        \n",
    "    for fileset, channel_hist in out.items():    \n",
    "        \n",
    "        for channel, histogram in channel_hist.items():\n",
    "            \n",
    "            # There is only one sample and one variation for each fileset in our data.\n",
    "            sample = histogram.axes[1][0]\n",
    "            variation = histogram.axes[2][0]\n",
    "            current_hist = histogram[:, sample,variation]\n",
    "\n",
    "            histograms_collection.append({\"sample\": sample, \"histogram\": current_hist, \"channel\":channel, \"variation\":variation})\n",
    "            \n",
    "\n",
    "    samples_uique = {el[\"sample\"] for el in histograms_collection}\n",
    "    channels_unique = {el[\"channel\"] for el in histograms_collection}\n",
    "    variations_unique = {el[\"variation\"] for el in histograms_collection}\n",
    "    \n",
    "    for channel in channels_unique:\n",
    "\n",
    "        pseudodata_hist = 0\n",
    "        \n",
    "        for variation in variations_unique:\n",
    "        \n",
    "            for sample in samples_uique:\n",
    "\n",
    "                merged_hist = 0\n",
    "\n",
    "                for el in histograms_collection:\n",
    "\n",
    "                    if el[\"sample\"] == sample and el[\"channel\"] == channel and el[\"variation\"] == variation:\n",
    "\n",
    "                        merged_hist += el[\"histogram\"]\n",
    "                \n",
    "                if merged_hist != 0: # if there is histogram with combintaion of such sample and variation\n",
    "                    f[f\"{channel}/{sample}/{variation}\"] = merged_hist\n",
    "\n",
    "        \n",
    "        for el in histograms_collection:\n",
    "            \n",
    "            if el[\"channel\"] == channel and el[\"variation\"] == \"nominal\":\n",
    "                \n",
    "                pseudodata_hist += el[\"histogram\"]        \n",
    "        \n",
    "        pseudodata_hist.view().value += np.random.normal( scale = np.sqrt(pseudodata_hist.values()) )\n",
    "        \n",
    "        f[f\"{channel}/pseudodata/nominal\"] = pseudodata_hist\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babc372-9961-4ca8-849b-e98cdc10bfb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear up, in case using cells below repetitively\n",
    "!rm -f histograms/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030e481-d800-4afe-adf9-753dea057db2",
   "metadata": {},
   "source": [
    "The statistical model for fitting the pseudostate is defined in the file `cabinetry_config.yml`. The code below reads the configuration file and builds the `pyhf` workspace.\n",
    "\n",
    "### Notions on sample names in the cabinetry config file.\n",
    "\n",
    "The names of the samples (which correspond to physical processes) in the .yml contain a number at the beginning of the string. This is done because the stacked histogram in the visualisation of cabinetry results is built from the bottom to the top in alphabetic order. By adding numbers to the sample names one could control the order in which histograms are visualised and make it according to their preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cb5f1-adba-4eb1-9b53-3c9ce5a1c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry_config = cabinetry.configuration.load(\"cabinetry_config.yml\")\n",
    "cabinetry.templates.collect(cabinetry_config, method=\"uproot\")\n",
    "cabinetry.templates.postprocess(cabinetry_config)\n",
    "\n",
    "workspace_path = \"workspaces/example_workspace.json\"\n",
    "ws = cabinetry.workspace.build(cabinetry_config)\n",
    "cabinetry.workspace.save(ws, workspace_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841391df-f128-4e26-8cd3-5c5a2492f3c5",
   "metadata": {},
   "source": [
    "Let's explore the workspace with `pyhf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32882d2-c5f7-4605-9fa1-fe9108ca0376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pyhf inspect workspaces/example_workspace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b850e-8bd4-4f6d-9871-aba0d543ab54",
   "metadata": {},
   "source": [
    "The next step is performing minimum likelihood fitting of our statistical model to the pseudo data histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08add32-997a-488f-ab35-1fd7ba1ac27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "cabinetry.visualize.pulls(\n",
    "    fit_results, close_figure=True, save_figure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bca47-6bd4-4cec-bf69-256eb8acc894",
   "metadata": {},
   "source": [
    "The cell below prints the results of the fit on the `ttbar_norm` parameter as defined in the statistical model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a92331-c87c-4a3f-963e-35d0a3fdd5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poi_index = model.config.poi_index\n",
    "print(f\"\\nfit result for ttbar_norm: {fit_results.bestfit[poi_index]:.3f} +/- {fit_results.uncertainty[poi_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7de24d-e81b-493b-91ea-0dee4195c66c",
   "metadata": {},
   "source": [
    "Let's visualise the model for both observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad54e9-f458-4ed7-84f9-611d74be409a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "model_prediction_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, config=cabinetry_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42bf076-7ba5-45ef-a93a-61a59e52759b",
   "metadata": {},
   "source": [
    "The histograms above have numbers in the legend which occurs as the result of using numbers to order the building of the histograms. The code in the next cell changes the label names by applying the matplotlib functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350d614-05c8-480c-926e-2a35507fce07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over figures. The different observables corresponds to different figures.\n",
    "for fig in figs:\n",
    "    \n",
    "    # Get a list of all axes in the figure\n",
    "    axes = [ax for ax in fig[\"figure\"].get_axes()]\n",
    "    \n",
    "    # Get a list of all legends in the figure\n",
    "    legends = [ax.get_legend() for ax in axes if ax.get_legend() is not None]\n",
    "    \n",
    "    for legend in legends:\n",
    "    \n",
    "        # Iterate over text entries into legends\n",
    "        for text in legend.get_texts():\n",
    "\n",
    "            # Get the actual text of the label in str format\n",
    "            label = text.get_text()\n",
    "            # Remove numbers from the label\n",
    "            new_label = re.sub(r'\\d{1,3}\\)\\s', '', label)\n",
    "            # Write a new string label to be the entry of the legend\n",
    "            text.set_text(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e97a2-ec02-4633-a54d-d4cf0d60cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49256c-a303-4f73-815d-45a54b2dcc87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figs[1][\"figure\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

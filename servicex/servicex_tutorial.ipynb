{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"><img src=\"images/uw.png\" width=\"90\" height=\"90\" style=\"float:left\" alt=\"ServiceX\"><img src=\"images/logo_servicex.png\" width=\"90\" height=\"90\" style=\"float:right\" alt=\"IRIS-HEP\">IRIS-HEP ServiceX Tutorial</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\">Artur Cordeiro Oudot Choi (University of Washington)</h4><h4 style=\"text-align: center;\">IRIS-HEP Analysis Software Training Event (July 2025)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 – Introduction\n",
    "\n",
    "This notebook starts with a presentation of ServiceX and its benefits compared to some current ATLAS software resources for some frequent tasks. The second part of the notebook will give showcase ServiceX features and examples on how to use it. \n",
    "At the end of the notebook, you are encouraged to try out servicex and follow a recommended exercise. \n",
    "\n",
    "#### **The Worldwide LHC Computing Grid**\n",
    "\n",
    "  The WLCG, Grid for short, provides computing resources for storage and data-analysis for LHC experiments. It is spread across 170 sites in 42 countries, utilizing CERN and national institutes' infrastructure. The grid stores ATLAS's collected experimental data and centrally produced MC simulations but also user-produced datasets (custom derivations, skimmed Ntuples, private simulations).\n",
    "\n",
    "  The Grid is a crucial tool for ATLAS to:\n",
    "\n",
    "  - deal with the total file-size of datsets that can easily saturate any user storage\n",
    "  - allow frequent dataset access worldwide\n",
    "  - Provide computing power in a batch system for heavy tasks: computing systematic errors, doing simulations, running reconstruction algorithms\n",
    "\n",
    "However, using the Grid requires many different and sometimes complex ATLAS tools. This presents barriers for simple dataset exploration task or R&D on targeted features that do not require large computing power. \n",
    "\n",
    "#### **Managing Datasets for analyses**\n",
    "\n",
    "  When working with the Grid within an analysis group as privately there are many points to include in your workflow.\n",
    "  \n",
    "  The current DS management workflow can include:\n",
    "  - Official requests for large storage spaces for each analysis team (`eos`, `groupsdisk`)\n",
    "  -  Institute-provided machines $\\star$\n",
    "      - storage + access to ATLAS tools  \n",
    "  - Sending ntuplizer jobs on the grid to skim DS $\\star$\n",
    "      -  event pre-selection, triggers, working points, High-level variable construction, dropping low-level information\n",
    "      -  Often requires Athena or AnalysisBase\n",
    "          - Different releases require different setups, containers   \n",
    "      -  custom reconstructions ...\n",
    "      -  Done using `panda`\n",
    "          -  Difficult debugging\n",
    "          -  Difficult access to logs\n",
    "          -  different protocols for different types of tasks\n",
    "  -  Downloading full or partial DS, skimmed Ntuples with `rucio` $\\star$\n",
    "  - Managing all productions\n",
    "      - Deleting older files\n",
    "      - requesting DS persistence exceptions\n",
    "      - requesting larger group storage\n",
    "\n",
    "<break>\n",
    "    \n",
    "    \n",
    "### **What is ServiceX?**\n",
    "\n",
    "\n",
    "ServiceX is a modern tool that manages to replace parts of this workflow ($\\star$), aiming to simplify frequent light R&D tasks that usualy require the Grid.\n",
    "\n",
    "\n",
    "<h1 style=\"text-align: center;\"><img src=\"images/what-is-sx.jpeg\" alt=\"sx-benefits\" width=\"900\" height=\"100\"></h1>\n",
    "\n",
    "\n",
    "  ServiceX is a data transformation and delivery system that simplifies the data access workflow for physics analyses. The system is deployed on a Kubernetes cluster, allowing quick and scalable operations with parallel transformers and access to multiple services.\n",
    "  It is part of IRIS-HEP DOMA (Data Organization, Management And Access) and aims to interface datasets and user-level analysis code for the HL-LHC in a Python environment.\n",
    "  \n",
    "#### **ServiceX main use cases**\n",
    "\n",
    "  \n",
    "  - Pull only the columns or features needed without having to download full files\n",
    "  -  Apply cuts to data, delivering only a portion of events to:\n",
    "      - Quickly prototype cutflows\n",
    "      - Target studies on unusual events, calibration errors...\n",
    "      - No need to sumbmit `panda` jobs to run on lxplus on downloaded samples\n",
    "  -  Integrate analyses in a Python-based framework (`coffea`, `RDataFrame`, `awkward`)\n",
    "  -  Work with multiple data formats on the same system (xAOD, DAOD, Ntuple Trees, RNTP, parquet)\n",
    "  -  Access to multiple releases of ATLAS AnalysisBase\n",
    "  -  Access different remote storage (`rucio`, `eos`, `CernOpenData`)\n",
    "  -  Setup a lightweight python package locally instead of ATLAS software environment on remote machines.\n",
    "  -  Write data transformations in a few lines of code with declarative syntax \n",
    "\n",
    "<break>  \n",
    "    \n",
    "#### **How does it work?**\n",
    "- <u>Query-based data transformation and delivery<u>\n",
    "\n",
    "  Users send HTTPS requests to the ServiceX Web API using the [client library](https://github.com/ssl-hep/ServiceX_frontend). Each request contains an input sample ID/path and all data transformations to be applied and dumped to the result file.\n",
    "\n",
    " - <u>Remote data access<u>\n",
    "\n",
    "   After receiving a request, the Web API sends the requested sample name to a Dataset Finder service which finds a way to get it via the Grid, XRootD or CernOpenPortal. ServiceX can also be deployed on a grid site which would maximize bandwidth to get samples faster. \n",
    "\n",
    " - <u>Code Generation<u>\n",
    "\n",
    "\n",
    "   Executable code is generated from the query specifications sent to the Web API. ServiceX builds this Python or C++ code by leveraging abstract syntax trees and functional programming. This means users can express complex data processing in a few lines. Multiple query types are available with different syntax/code translation.\n",
    "\n",
    " - <u>Transformers and science images<u>\n",
    "\n",
    "   To execute the generated code, the kubernetes cluster deploys transformer pods. Depending on the request, these pods require different environments. Each has its science image used to build containers, and they can include: python environements with awkward, uproot, RDataFrame and Root or Atlas software like AnalysisBase or TopCPToolkit...  The number of transformers deployed for one request scales with the number of files to be acessed in the requested dataset. By parallelizing the work on multiple files and merging the outputs, servicex delivers the results fast. \n",
    "\n",
    " - <u>Object store<u>\n",
    "\n",
    "   Transformed outputs are written to an S3-compatible store; users download just URLs or files as they become available.\n",
    "\n",
    "<h1 style=\"text-align: center;\"><img src=\"images/ServiceXDiagram2.png\" alt=\"sx-benefits\" width=\"1000\" height=\"100\"></h1>\n",
    "\n",
    "\n",
    "<break>\n",
    " \n",
    "#### **Minimal setup**\n",
    "\n",
    "One great advantage of ServiceX is it's minimal setup. Most of the work happens in the backend thus leaving the front-end (user-facing) side very light. \n",
    "\n",
    "- ServiceX Front-end: A minimal Python library (client API), for defining queries and sending HTTPS queries to the web api\n",
    "- ServiceX Back-end: The kubernetes system containing all microservices in static pods and transformer pods that interpret the queries, execute the data transformation, and store it to an S3 store. This is deployed at an analysis facility. \n",
    "\n",
    "  As a user you only interact with the client API (python script or with command lines) but you need to get token that grants access to the back-end server. \n",
    "\n",
    "This simple setup means that you can run ServiceX from anywhere: laptops, office machine, jupyter hub... \n",
    "\n",
    "For any information on the client API please check the [documentation](https://servicex-frontend.readthedocs.io)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this notebook is a hands-on tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 – Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a - Client package\n",
    "\n",
    "\n",
    "The [servicex](https://github.com/ssl-hep/ServiceX_frontend/tree/master) Pypi package is the only API you need to send queries and retrieve results. \n",
    "Installation bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install servicex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b - Utilities package\n",
    "\n",
    "\n",
    "In parallel, a [serivex_analysis_utils](https://github.com/ssl-hep/ServiceX_analysis_utils/tree/main) library is also in development to put together Servicex usability helpers and specific tools that use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],

   "source": [
    "!pip install servicex-analysis-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c - Access to endpoint\n",
    "\n",
    "For ATLAS the back-end instance you will use is [af.uchicago](https://servicex.af.uchicago.edu/)\n",
    "\n",
    "You need to login using your ATLAS credentials (Cern SSO), got to your profile, and create/download the token `servicex.yaml`. <br>\n",
    "Be sure to put this config file in your running path.\n",
    "\n",
    " <h1 style=\"text-align: center;\"><img src=\"images/token.jpeg\" alt=\"token\" width=\"600\" height=\"50\"></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.path.exists('servicex.yaml'), 'servicex.yaml not found in working directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 – Building Your First ServiceX Request\n",
    "\n",
    "There are multiple transformer types that ServiceX's backend can deploy. Each one requires a different type of query that is constructed using different syntaxes. <be>\n",
    "Bellow you can find a table of currently supported transformations, their file formats (input and output), and the query type you need to use with ServiceX to send a request.\n",
    "\n",
    " <h1 style=\"text-align: center;\"><img src=\"images/available-transforms.jpeg\" alt=\"trans-types\" width=\"600\" height=\"50\"></h1>\n",
    "\n",
    "\n",
    "We will start by building one intuitive query for the `python` transformation. For this, you write down the actual function that is executed by the transformers. <br> \n",
    "The transformation request will consist on applying selection cuts on truth variables and reco objects, then it will dump the truth decay vertex of a BSM particle (axion-like-particle) and the EM fraction of reco jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed functions \n",
    "from servicex import deliver, query, dataset\n",
    "from servicex_analysis_utils import to_awk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Runing a python function in the backend\n",
    "ServiceX provides a trasnformer type that distributes a user-written python function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a - Building the query event selection\n",
    "The python environment has access to all standard python libraries plus `uproot` and `awkward`. The user imports must be done within the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The user function\n",
    "\n",
    "def run_query(input_filenames):\n",
    "    import uproot\n",
    "    import awkward as ak\n",
    "    \"\"\"\n",
    "    input_filenames : \n",
    "        The dataset specified in the query will be loaded here\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ak.Array with fields truth_alp_decayVtxX, truth_alp_decayVtxY, jet_pt_NOSYS\n",
    "    for events passing the selection cuts. The array will be saved in a file and sent to ServiceX's object store\n",
    "    \"\"\"\n",
    "    # 1) Load the branches into a single awkward array with uproot\n",
    "    treename=\"reco\"\n",
    "    branches = [\n",
    "        \"truth_alp_decayVtxX\",\n",
    "        \"truth_alp_decayVtxY\",\n",
    "        \"truth_alp_pt\",\n",
    "        \"truth_alp_eta\",\n",
    "        \"jet_EMFrac_NOSYS\", \n",
    "        \"jet_pt_NOSYS\",\n",
    "    ]\n",
    "    \n",
    "    with uproot.open(input_filenames) as f:\n",
    "        tree = f[treename]\n",
    "        arr = tree.arrays(branches, library=\"ak\")\n",
    "\n",
    "    # 2) Build the mask (cuts you want to apply)\n",
    "    mask = ak.ravel(\n",
    "        (abs(arr[\"truth_alp_eta\"]) < 0.8) &\n",
    "        (arr[\"truth_alp_pt\"] > 20) &\n",
    "        (ak.num(arr[\"jet_EMFrac_NOSYS\"]) < 2)\n",
    "    )\n",
    "\n",
    "    # 3) Apply the mask and zip only the three output fields you need\n",
    "    out = ak.zip({\n",
    "        \"truth_alp_decayVtxX\": arr[\"truth_alp_decayVtxX\"][mask],\n",
    "        \"truth_alp_decayVtxY\": arr[\"truth_alp_decayVtxY\"][mask],\n",
    "        \"jet_EMFrac_NOSYS\":       arr[\"jet_EMFrac_NOSYS\"][mask]\n",
    "    }, depth_limit=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the servicex query object\n",
    "python_query=query.PythonFunction().with_uproot_function(run_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b - Selecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTuple stored in a group disk at LPC (France)\n",
    "my_DS=\"user.acordeir:michigan-tutorial.displaced-signal.root\" \n",
    "#Use the rucio dataset-types for the DSFinder \n",
    "request_DS=dataset.Rucio(my_DS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c - Build the ServiceX request spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple query\n",
    "spec = {\n",
    "    'Sample': [{\n",
    "        'Name': 'My_first_transform',\n",
    "        'Dataset': request_DS,\n",
    "        'Query': python_query\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d - Send the request to the Web API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliver the request\n",
    "transformed_results = deliver(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitor transforms** <br>\n",
    "By accessing the ServiceX [dashboard](https://servicex.af.uchicago.edu/dashboard) you should see your first request `'My_first_transform'` . <be>\n",
    "From this page you can get specific details about your request, such as IDs to help navigate logs (later in the tutorial). You can also cancel transformations from here if you realise a job should be killed. It is good practice to avoid taking resources unecessarly, and cancelling the deliver function will not stop a request that was sent to the backend.\n",
    "\n",
    " <h1 style=\"text-align: center;\"><img src=\"images/monitoring.png\" alt=\"monitoring\" width=\"800\" height=\"80\"></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, ServiceX results from `deliver()` are downloaded from the object store into the user's Local Cache `tmp/servicex_$user/`. When re-submitting a transformation request, the cache will be checked for any existing results (Local and S3 store). This prevents re-processing transformations that are already accessible, and becomes very useful when a user adds extra samples to an existing deliver `spec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deliver returns a dict with a path to the result\n",
    "print(transformed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Quick analysis example \n",
    " ServiceX data delivery is particularly usefull for fast dataset exploration and specific studies. It is straigthforward to integrated it in small python scripts that analyse some physics features. \n",
    " <br>\n",
    " Here we will define and run a short script that computes the displacement of the ALP and plots it in a histogram along with the jet EM fraction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "\n",
    "def analyse(results, name):\n",
    "    # extract your array with analysis_utils\n",
    "    arr = to_awk(results)[name]\n",
    "\n",
    "    # compute displacement \n",
    "    displacement = (arr[\"truth_alp_decayVtxX\"]**2 +\n",
    "                    arr[\"truth_alp_decayVtxY\"]**2)**0.5\n",
    "\n",
    "    # flatten the awkward arrays\n",
    "    emfrac = ak.flatten(arr[\"jet_EMFrac_NOSYS\"])\n",
    "    disp   = ak.flatten(displacement)\n",
    "\n",
    "    # set up side-by-side subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "    # EM fraction\n",
    "    axes[0].hist(emfrac, bins=50, range=[0,1] )\n",
    "    axes[0].set_title(f\"{name}: Jet EM Fraction\")\n",
    "    axes[0].set_xlabel(\"EM Fraction\")\n",
    "    axes[0].set_ylabel(\"Counts\")\n",
    "\n",
    "    # displacement\n",
    "    axes[1].hist(disp, bins=50, range=[0,5000], color=\"g\")\n",
    "    axes[1].set_title(f\"{name}: Decay Vertex Displacement\")\n",
    "    axes[1].set_xlabel(\"Displacement in x-y (mm)\")\n",
    "    axes[1].set_ylabel(\"Counts\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(transformed_results, \"My_first_transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast results from remote files\n",
    "\n",
    "This is a standard demonstration of how a physicist can include `servicex` in frequent workflows.\n",
    "\n",
    "\n",
    "From this notebook we managed to get histograms with releavant physics features extremely fast. All it took was to import 4 libraries and run 8 short cells. Plus this can be ran from everywhere on a laptop. \n",
    "<br>\n",
    "To recap, obtaining such a result with standard tools would require: login to `lxplus` (or other), setupATLAS tools, validate Grid credentials, use `rucio` to download the DS, run your root macro or python script to get the histograms. \n",
    "\n",
    "<break>\n",
    "\n",
    "If you consider another transformation (that ServiceX can do), and that would normaly require `Athena`/`AnalysisBase`, and the submission of Grid jobs, the difference would be even more blatant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 – Other Transformer Types for root files\n",
    "\n",
    "We will implement the same transformation but using other query types. ServiceX offers queries with declarative syntax to further simplify the user input. For these queries, the backend codegen will significantly expand the writen query to a standard C++ or Python code.  \n",
    "\n",
    "### 4.1- UprootRaw \n",
    "This backend essentially executes a python script with uproot and awkward but uses functional programing. \n",
    "\n",
    "For a standard skimming, you must provide:\n",
    "- `treename`: the TTree name you want to open \n",
    "- `filter_bame`: a list of branches to load from that Tree\n",
    "- `cut`: The selection that you want to apply. It is done by using awkward methods such as:  `num, count, count_nonzero, any, all, sum, prod, min, max, argmin, argmax, mean, var, std`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the same selection as above\n",
    "uproot_raw_query=query.UprootRaw([{\n",
    "    'treename':'reco',\n",
    "    'filter_name':\n",
    "      [ \"truth_alp_decayVtxX\",\n",
    "        \"truth_alp_decayVtxY\",\n",
    "        \"truth_alp_pt\",\n",
    "        \"truth_alp_eta\",\n",
    "        \"jet_EMFrac_NOSYS\", \n",
    "        \"jet_pt_NOSYS\" ],\n",
    "    'cut':\n",
    "        '(num(jet_pt_NOSYS)<2) & any((truth_alp_pt>20) & (abs(truth_alp_eta)<0.8))'}])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the trasnform specification\n",
    "spec_upraw = {\n",
    "    'Sample': [{\n",
    "        'Name': 'UprootRawExample',\n",
    "        'Dataset': request_DS,\n",
    "        'Query': uproot_raw_query\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_uproot_raw=deliver(spec_upraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(results_uproot_raw, \"UprootRawExample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We obtain the exact same result, in a much shorter query definition!** <br>\n",
    "However, not being able to explicitly chose what gets saved, all loaded branches are sent to the result file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2- Func ADL \n",
    "Functional Analysis Description Language, based on LINQ, is an SQL-like language used to construct hierarchical data queries for servicex. \n",
    "Func_adl it-self contains multiple backend options. It is written to work with different transformer types that use different environment builds. <br>\n",
    "\n",
    "From the servicex client package one can directly build func_adl queries for different software types.<br>\n",
    "Native `funcADL` backends in servicex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(query.plugins)[0:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building blocks\n",
    "FuncADL analysis language is made by multiple building block that can be combined in different ways to write a transformation. \n",
    "<br> The two main blocks are: \n",
    "- *Select* to select an object from the DS onto which further manipulations can be done, or to dump an object to the saved file with custom branch names. <br> Generally, it is analogous to opening a loop or writing out objects.\n",
    "-  *Where* to apply the cuts, written as functions returning booleans (can be lambda functions) <br> It is analogous to an if statement within a loop. \n",
    "\n",
    "A list of all available building blocks can be found in [LINQ's Wikipedia](https://en.wikipedia.org/wiki/Language_Integrated_Query)<br>\n",
    "\n",
    "Wrapping all these methods, with different nesting levels, enables a high modularity in the way you can express a transformation. \n",
    "This also allows you to build complex transformations on different separate objects with explicit \"loop levels\" and functions for event selection or variable building.\n",
    "\n",
    "**Example with the FuncADL UpRoot backend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The selection functions must return a boolean\n",
    "def jet_cut(evt) -> bool:\n",
    "    return evt[\"jet_pt_NOSYS\"].Count() < 2\n",
    "\n",
    "def alp_pt_cut(evt) -> bool:\n",
    "    # at least one ALP with pT > 20 GeV\n",
    "    return evt[\"truth_alp_pt\"].Where(lambda pt: pt > 20).Count() > 0\n",
    "\n",
    "def alp_eta_cut(evt) -> bool:\n",
    "    # at least one ALP with |η| < 0.8\n",
    "    return evt[\"truth_alp_eta\"].Where(lambda eta: abs(eta) < 0.8).Count() > 0\n",
    "\n",
    "simple_query = query.FuncADL_Uproot() \\\n",
    "    .FromTree(\"reco\") \\\n",
    "    .Where(jet_cut) \\\n",
    "    .Where(alp_pt_cut) \\\n",
    "    .Where(alp_eta_cut) \\\n",
    "    .Select(lambda e: {\n",
    "        \"jet_EMFrac_NOSYS\":  e[\"jet_EMFrac_NOSYS\"],\n",
    "        \"truth_alp_decayVtxX\":  e[\"truth_alp_decayVtxX\"],\n",
    "        \"truth_alp_decayVtxY\": e[\"truth_alp_decayVtxY\"],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_funcadl = {\n",
    "    'Sample': [{\n",
    "        'Name': 'FuncADLExample',\n",
    "        'Dataset': request_DS,\n",
    "        'Query': simple_query\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_funcadl=deliver(spec_funcadl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(result_funcadl, \"FuncADLExample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to obtain the same results using this servicex functional programming query type. The syntax is slightly more complex than the `UprootRaw` queries. However, its modularity allows for much better structuration of complex transformations into a single query. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - ServiceX and ATLAS software for xAOD\n",
    "\n",
    "The FuncADL query syntax becomes a great tool when one considers the ATLAS software backend. For such transformations, the `Select` and `Where` methods grant access to xAOD objects, their decorators, links, methods, CP tools... \n",
    "<br>\n",
    "\n",
    "This notebook section showcases how to manipulate xAOD objects, like `TruthParticle` containers, as you would in `Athena` or `AnalysisBase` directly reading them from `DAOD` files. Further documentation and examples can be found in the [IRIS-HEP](https://iris-hep.org/xaod_usage/intro.html) webpage\n",
    "<break>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLP1 DAOD\n",
    "#Similar signal to what was in the Ntuple\n",
    "DAOD=\"mc23_13p6TeV:mc23_13p6TeV.601806.PhPy8EG_AZNLO_ggH125_mA0p4_medium.deriv.DAOD_LLP1.e8526_e8528_s4111_s4114_r14622_r14663_p6368\"\n",
    "request_daod=dataset.Rucio(DAOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_adl = query.FuncADL_ATLASr22()\n",
    "#Building the TruthParticle_V1 xAOD object and filling it with the ALPs\n",
    "truth_alp = query_adl.Select(\n",
    "    lambda e: e\n",
    "        .TruthParticles(\"TruthBSMWithDecayParticles\")\n",
    "        .Where(lambda p: p.absPdgId() == 35)\n",
    ")\n",
    "\n",
    "# Compute and write out the displacement by accessing TruthParticle decorators\n",
    "truth_info = truth_alp.Select(\n",
    "    lambda ts: {\n",
    "        \"LLP_Lxy\": ts.Select(lambda p: (p.decayVtx().x()**2 + p.decayVtx().y()**2)**0.5),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the request specification\n",
    "xAOD_spec = {\n",
    "    \"Sample\": [\n",
    "        {\n",
    "            \"Name\": \"func_adl_xAOD_simple\",\n",
    "            \"Dataset\": request_daod,\n",
    "            \"Query\": truth_info,\n",
    "            \"NFiles\": 1,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "xAOD_results = deliver(xAOD_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we downloaded the result, lets check some features of the displacement distribution for this signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the result and compute the percentage of in-tracker decays \n",
    "Lxy=to_awk(xAOD_results)[\"func_adl_xAOD_simple\"][\"LLP_Lxy\"]\n",
    "print(\n",
    "    f\"The BSM particle has a truth displacement mean of {ak.mean(Lxy): .2f} mm \\\n",
    "    \\nApproximately {ak.count_nonzero(Lxy<1000)/len(Lxy)*100 :.1f} % of events have secondary vertices in ATLAS's tracker area\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:**\n",
    "\n",
    "The standard workflow to reach this result with a DAOD would involve: setting up and writing an AnalysisBase or Athena package in C++, building the `initialize()`, `execute()` and `finalize()` functions, access the objects with specific handles, do the computation on the xAOD methods, compile everything in the correct release, write down a python config file with all the flags you need and the input file paths.     \n",
    "\n",
    "Additionally, for larger operations one might have to configure ATLAS `panda` to distribute the job on the GRID.\n",
    "\n",
    "\n",
    "With FuncADL, ServiceX offers an alternative to the standard usage of ATLAS software. Physicists can now do this type of R&D study much quickly and from their laptops. Thanks to the modular aspect of the syntax, more complex transforms can also be written. \n",
    "<br> \n",
    "\n",
    "Here is an [example of a large transformation](https://github.com/gordonwatts/sx_training_fetch/blob/main/calratio_training_data/training_query.py) that takes advantage of the xAOD backend to build ML training datasets directly from a derivation file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Some usefull functionalities \n",
    "In this section we will quickly showcase other utilities that can improve your `ServiceX` based workflow.\n",
    "### 6.1 Sending transformation requests\n",
    "- Request specifications: \n",
    "    - `General` block\n",
    "        - `OutputFormat` for root-file, root-rntuple or parquet\n",
    "        - `Delivery` to download files to LocalCache or get SignedURLs to the S3 store\n",
    "        - Multiple `Sample` sub-blocks can be added. This allows different transforms per deliver call.<br>\n",
    "\n",
    "    - `Sample` sub-block\n",
    "        - `NFiles` to choose the number of files to transform for each dataset\n",
    "        - `Dataset` there are other types of support DS types\n",
    "        \n",
    "    - `UprootRaw` queries\n",
    "        - `copy_histogram` This transformer type can directly copy Root histograms from the original file\n",
    "      \n",
    "\n",
    " \n",
    "<break>\n",
    "        \n",
    "- `deliver` function\n",
    "\n",
    "  \n",
    "    - `backend`: the end-point server can be manually selected\n",
    "    - `config_path`: the path to your servicex.yaml token\n",
    "    - `spec_file`: Can be used to import the transform specification from a .yaml file\n",
    "    -  Command line version for sending ServiceX trasnfoms from the shell\n",
    "\n",
    "#### Command line example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Executing from the terminal shell with spec file\n",
    "!servicex deliver spec_file_example.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See how the transform is written\n",
    "#muon selection with jet tagging cuts\n",
    "!cat spec_file_example.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ServiceX errors and logs \n",
    "As the transformations run in the backend, the user does not have live access to the transformer logs. \n",
    "However, when a transformation fails, ServiceX will send the log body to a Kibana instance that can be accessed in a Browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of error in transformer execution\n",
    "\n",
    "def run_query(input_filenames):\n",
    "    return non_defined_variable\n",
    "\n",
    "error_query=query.PythonFunction().with_uproot_function(run_query)\n",
    "\n",
    "spec = {\n",
    "    \"Sample\": [\n",
    "        {\n",
    "            \"Name\": \"error_example\",\n",
    "            \"Dataset\": request_DS,\n",
    "            \"Query\": error_query,\n",
    "            \"NFiles\": 1,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "result_bug=deliver(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the logs\n",
    "After clicking the link printed by the deliver function you are directed to the kibana log errors for your request ID. A summary message is written but you can expand the error tab to find the `log_body` key. This is where the transformer logs can be found. \n",
    "\n",
    "\n",
    " <h1 style=\"text-align: center;\"><img src=\"images/error-log-hints.png\" alt=\"error\" width=\"800\" height=\"80\"></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Remote file introspection with ServiceX\n",
    "\n",
    "The `servicex_analysis_utils` package offers a function (`get_structure()`) that generates and sends a request to retrieve the metadata and file structure of a Rucio dataset. This tool, which can also be used on your terminal, becomes very useful whenever you need to know what is on a specific dataset. \n",
    "- For building ServiceX queries you need to know you dataset structure.\n",
    "    -  TTrees/branch names\n",
    "    -  available xAOD containers and decorations \n",
    "    -  Level of array nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from servicex_analysis_utils import get_structure\n",
    "\n",
    "structure_string=get_structure(my_DS, filter_branch=\"jet\")\n",
    "print(structure_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the full Tree structure of your dataset in a few seconds. Including the name of TTrees, TBranches, base types, and levels of nesting (Jaggness). \n",
    "\n",
    "It also allows you to check what are the available xAOD containers on AOD and DAOD files with branch name filtering. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 – Free hands-on  \n",
    "\n",
    "In this section you are encouraged to try ServiceX for any transformation you have in mind.\n",
    "\n",
    "- You can use either of the datasets `request_DS` or `request_daod`, or one that you might be familiar with.\n",
    "- Try to use different transformer types `PythonFunction`, `UprootRaw`, `Fund_ADL_Uproot`, `Func_ADL_xAOD`.\n",
    "- Remeber to use `get_structure` function to see what is available in the datasets.\n",
    "- Try to make the best use of declarative syntax with the functional language queries.\n",
    "- Some datasets have many files, you can set `NFiles` to 1 to quick test.\n",
    "\n",
    "Here you have exercise suggestions for the datasets used in this notebook.\n",
    "\n",
    " #### Exercise suggestion 1:\n",
    " - Compute the invariant mass of the 2-lepton system\n",
    " - Select events with exactly 2 opposite-charge electrons or muons\n",
    "\n",
    "\n",
    "Formula reminder for 2 reco objects:\n",
    "\n",
    " $\\quad\\quad$  $M^2 = 2 p_{T1} p_{T2} \\left( \\cosh(\\eta_1 - \\eta_2) - \\cos(\\phi_1 - \\phi_2) \\right)$\n",
    "\n",
    " #### Exercise suggestion 2:\n",
    "  - Compute the invariant mass of the lepton-lepton-jet (leading) system on the LLP datasets\n",
    "  - Implement a 40 GeV pt cut on the truth BSM particle (ALP)\n",
    "  - Select events with exactly 2 opposite-charge leptons\n",
    "  - Use Lorentz vectors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

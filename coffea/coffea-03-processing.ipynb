{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf4cfc8",
   "metadata": {},
   "source": [
    "# Coffea Processors\n",
    "Coffea relies mainly on [uproot](https://github.com/scikit-hep/uproot) to provide access to ROOT files for analysis.\n",
    "As a usual analysis will involve processing tens to thousands of files, totalling gigabytes to terabytes of data, there is a certain amount of work to be done to build a parallelized framework to process the data in a reasonable amount of time.\n",
    "\n",
    "In coffea up to 0.7 (SemVer), a `coffea.processor` module was provided to encapsulate the core functionality of the analysis, which could be run locally or distributed via a number of Executors. This allowed users to worry just about the actual analysis code and not about how to implement efficient parallelization, assuming that the parallization is a trivial map-reduce operation (e.g. filling histograms and adding them together).\n",
    "\n",
    "In coffa 2024 (CalVer), integration with `dask` is deeper (via `dask_awkward` and `uproot.dask`), and whether an analysis is to be executed on local or distributed resources, a TaskGraph encapsulating the analysis is created (with a bypass functionality for an eager version still possible when the scale of your input data is sufficiently small). We will demonstrate how to use callable code to build these TGs.\n",
    "\n",
    "(Sidenote: with some adaptations for the new version of scikit-hep/coffea, a SemVer coffea processor module's `process` function can serve as the callable function - we will follow this model for convenience as we are transitioning from SemVer to CalVer coffea)\n",
    "\n",
    "\n",
    "Let's start by writing a simple processor class that reads some CMS open data and plots a dimuon mass spectrum.\n",
    "We'll start by copying the [ProcessorABC](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html#coffea.processor.ProcessorABC) skeleton and filling in some details:\n",
    "\n",
    " * Remove `flag`, as we won't use it\n",
    " * Adding a new histogram for $m_{\\mu \\mu}$\n",
    " * Building a [Candidate](https://coffeateam.github.io/coffea/api/coffea.nanoevents.methods.candidate.PtEtaPhiMCandidate.html#coffea.nanoevents.methods.candidate.PtEtaPhiMCandidate) record for muons, since we will read it with `BaseSchema` interpretation (the files used here could be read with `NanoAODSchema` but we want to show how to build vector objects from other TTree formats)\n",
    " * Calculating the dimuon invariant mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import awkward as ak\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.methods import candidate\n",
    "from coffea.dataset_tools import apply_to_fileset, max_chunks, max_files, preprocess\n",
    "import dask\n",
    "from hist.dask import Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self, events):\n",
    "        _ = events.metadata[\"dataset\"]\n",
    "        muons = ak.zip(\n",
    "            {\n",
    "                \"pt\": events.Muon_pt,\n",
    "                \"eta\": events.Muon_eta,\n",
    "                \"phi\": events.Muon_phi,\n",
    "                \"mass\": events.Muon_mass,\n",
    "                \"charge\": events.Muon_charge,\n",
    "            },\n",
    "            with_name=\"PtEtaPhiMCandidate\",\n",
    "            behavior=candidate.behavior,\n",
    "        )\n",
    "\n",
    "        h_mass = (\n",
    "            Hist.new.StrCat([\"opposite\", \"same\"], name=\"sign\")\n",
    "            .Log(1000, 0.2, 200.0, name=\"mass\", label=\"$m_{\\mu\\mu}$ [GeV]\")\n",
    "            .Int64()\n",
    "        )\n",
    "\n",
    "        cut = (ak.num(muons) == 2) & (ak.sum(muons.charge, axis=1) == 0)\n",
    "        # add first and second muon in every event together\n",
    "        dimuon = muons[cut][:, 0] + muons[cut][:, 1]\n",
    "        h_mass.fill(sign=\"opposite\", mass=dimuon.mass)\n",
    "\n",
    "        cut = (ak.num(muons) == 2) & (ak.sum(muons.charge, axis=1) != 0)\n",
    "        dimuon = muons[cut][:, 0] + muons[cut][:, 1]\n",
    "        h_mass.fill(sign=\"same\", mass=dimuon.mass)\n",
    "\n",
    "        return {\n",
    "            \"entries\": ak.num(events, axis=0),\n",
    "            \"mass\": h_mass,\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb2dd7",
   "metadata": {},
   "source": [
    "If we were to just use bare uproot to execute this processor, we could do that with the following example, which:\n",
    "\n",
    " * Opens a CMS open data file\n",
    " * Creates a NanoEvents object using `BaseSchema` (roughly equivalent to the output of `uproot.lazy`)\n",
    " * Creates a `MyProcessor` instance\n",
    " * Runs the `process()` function, which returns our accumulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcache_caching_server = \"root://xcache.af.uchicago.edu:1094//\"\n",
    "mumu_data_filename = f\"{xcache_caching_server}root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, BaseSchema\n",
    "\n",
    "events = NanoEventsFactory.from_root(\n",
    "    {mumu_data_filename: \"Events\"},\n",
    "    entry_stop=10000,\n",
    "    metadata={\"dataset\": \"DoubleMuon\"},\n",
    "    schemaclass=BaseSchema,\n",
    "    delayed=True,\n",
    ").events()\n",
    "p = MyProcessor()\n",
    "task_graph = p.process(events)\n",
    "task_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(task_graph[\"mass\"], optimize_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eeb2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(task_graph[\"mass\"], optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "out, *_ = dask.compute(task_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_dir = Path().cwd() / \"plots\"\n",
    "plot_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "\n",
    "mplhep.style.use(mplhep.style.ATLAS)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "out[\"mass\"].plot1d(ax=ax)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend(title=\"Dimuon charge\")\n",
    "\n",
    "fig.savefig(plot_dir / \"dimuon_charge.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97463f",
   "metadata": {},
   "source": [
    "# Filesets\n",
    "We'll need to construct a fileset to run over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mumu_simulation_filename = f\"{xcache_caching_server}root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a6299",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_fileset = {\n",
    "    \"DoubleMuon\": {\n",
    "        \"files\": {\n",
    "            mumu_data_filename: \"Events\",\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"is_mc\": False,\n",
    "        },\n",
    "    },\n",
    "    \"ZZ to 4mu\": {\n",
    "        \"files\": {\n",
    "            mumu_simulation_filename: \"Events\",\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"is_mc\": True,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e9bde",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "There are dataset discovery tools inside of `coffea` to help construct such datasets. Those will not be demonstrated here. For now, we'll take the above `initial_fileset` and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_available, preprocessed_total = preprocess(\n",
    "    initial_fileset,\n",
    "    step_size=100_000,\n",
    "    align_clusters=None,\n",
    "    skip_bad_files=True,\n",
    "    recalculate_steps=False,\n",
    "    files_per_batch=1,\n",
    "    file_exceptions=(OSError,),\n",
    "    save_form=True,\n",
    "    uproot_options={},\n",
    "    step_size_safety_factor=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac94c89",
   "metadata": {},
   "source": [
    "# Preprocessed fileset\n",
    "Lets have a look at the contents of the `preprocessed_available` part of the fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03755f61",
   "metadata": {},
   "source": [
    "## Saving a preprocessed fileset\n",
    "We can use the `gzip`, `pickle`, and `json` modules/libraries to both save and reload datasets directly. We'll do this short example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aafc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset_dir = Path().cwd() / \"filesets\"\n",
    "fileset_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"example_fileset\"\n",
    "with gzip.open(fileset_dir / f\"{output_file}_available.json.gz\", \"wt\") as file:\n",
    "    json.dump(preprocessed_available, file, indent=2)\n",
    "    print(f\"Saved available fileset chunks to {output_file}_available.json.gz\")\n",
    "with gzip.open(fileset_dir / f\"{output_file}_all.json.gz\", \"wt\") as file:\n",
    "    json.dump(preprocessed_total, file, indent=2)\n",
    "    print(f\"Saved complete fileset chunks to {output_file}_all.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004b3f2",
   "metadata": {},
   "source": [
    "We could then reload these filesets and quickly pick up where we left off. Often we'll want to preprocess again \"soon\" before analyzing data because this will let us catch which files are accessible now and which are not. The saved filesets may be useful for tracking, and we may have enough stability to reuse it for some period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(fileset_dir / f\"{output_file}_available.json.gz\", \"rt\") as file:\n",
    "    reloaded_available = json.load(file)\n",
    "with gzip.open(fileset_dir / f\"{output_file}_all.json.gz\", \"rt\") as file:\n",
    "    reloaded_all = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f76b4",
   "metadata": {},
   "source": [
    "# Slicing chunks and files\n",
    "Given this preprocessed fileset, we can test our processor on just a few chunks of a handful of files. To do this, we use the `max_files` and `max_chunks` functions from the dataset tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee379c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed_files = max_files(preprocessed_available, 1)\n",
    "test_preprocessed = max_chunks(test_preprocessed_files, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd969c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_task_graph, small_rep = apply_to_fileset(\n",
    "    data_manipulation=MyProcessor(),\n",
    "    fileset=test_preprocessed,\n",
    "    schemaclass=BaseSchema,\n",
    "    uproot_options={\"allow_read_errors_with_report\": (OSError, KeyError)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(small_task_graph, optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_computed, small_rep_computed = dask.compute(small_task_graph, small_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_rep_computed[\"DoubleMuon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_computed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e441a",
   "metadata": {},
   "source": [
    "Now, if we want to use more than a single core on our machine, we simply change [IterativeExecutor](https://coffeateam.github.io/coffea/api/coffea.processor.IterativeExecutor.html) for [FuturesExecutor](https://coffeateam.github.io/coffea/api/coffea.processor.FuturesExecutor.html), which uses the python [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) standard library. We can then set the most interesting argument to the `FuturesExecutor`: the number of cores to use (2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_task_graph, rep = apply_to_fileset(\n",
    "    data_manipulation=MyProcessor(),\n",
    "    fileset=preprocessed_available,\n",
    "    schemaclass=BaseSchema,\n",
    "    uproot_options={\"allow_read_errors_with_report\": (OSError, KeyError)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "out, rep = dask.compute(full_task_graph, rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80acc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe468e",
   "metadata": {},
   "source": [
    "Hopefully this ran faster than the previous cell, but that may depend on how many cores are available on the machine you are running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ec315",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = Path().cwd() / \"plots\"\n",
    "plot_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0885bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mplhep.style.use(mplhep.style.ATLAS)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "out[\"DoubleMuon\"][\"mass\"].plot1d(ax=ax)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.legend(title=\"Dimuon charge\")\n",
    "\n",
    "fig.savefig(plot_dir / \"dimuon_charge.png\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
